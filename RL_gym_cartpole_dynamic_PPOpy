import gymnasium as gym
import numpy as np
import math
from gymnasium.envs.classic_control.cartpole import CartPoleEnv
from gymnasium import spaces

class DynamicTargetCartPole(CartPoleEnv):
    def __init__(self, render_mode=None):
        super().__init__(render_mode=render_mode)
        self.time_step = 0
        self.x_target_base = 0.0
        self.x_target_amplitude = 1.5
        self.x_target_period = 200  # Steps per full sine wave

        # Extend the observation space to include x_target
        low = np.append(self.observation_space.low, -self.x_threshold)
        high = np.append(self.observation_space.high, self.x_threshold)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)

    def get_target_x(self):
        # Sinusoidal moving target
        return self.x_target_base + self.x_target_amplitude * math.sin(
            2 * math.pi * self.time_step / self.x_target_period
        )

    def step(self, action):
        self.time_step += 1
        self.target_x = self.get_target_x()

        obs, _, terminated, truncated, info = super().step(action)
        x, x_dot, theta, theta_dot = obs

        # Custom reward function
        distance = abs(x - self.target_x)
        reward = 1.0 - 0.2 * distance - 0.5 * abs(theta)
        reward = max(reward, 0.0)

        # Custom termination
        custom_terminated = (
            x < -self.x_threshold or x > self.x_threshold or
            theta < -self.theta_threshold_radians or theta > self.theta_threshold_radians
        )

        # Extended observation includes the current x_target
        extended_obs = np.append(obs, self.target_x).astype(np.float32)

        # For visualization/logging
        info["target_x"] = self.target_x

        return extended_obs, reward, custom_terminated, truncated, info

    def reset(self, *, seed=None, options=None):
        self.time_step = 0
        obs, info = super().reset(seed=seed, options=options)
        self.target_x = self.get_target_x()
        extended_obs = np.append(obs, self.target_x).astype(np.float32)
        return extended_obs, info

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# Register the environment using a function (for DummyVecEnv compatibility)
def make_env():
    return DynamicTargetCartPole(render_mode=None)


def training_session(timesteps:int = 10000):
    vec_env = DummyVecEnv([make_env])

    # Train PPO on this dynamic task
    model = PPO("MlpPolicy", vec_env, verbose=1)
    model.learn(total_timesteps=timesteps)

    # Save the trained model
    model.save("ppo_dynamic_target_cartpole")
    
    return(model)


def load_model(path2model):
    # Load the model from file
    model = PPO.load(path2model)
    return model

def run_model(model,iterations:int = 100):
    env = DynamicTargetCartPole(render_mode="human")
    obs, info = env.reset()

    for _ in range(iterations):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)

        print(f"Cart x = {obs[0]:.2f} | Target x = {obs[4]:.2f} | Reward = {reward:.2f}")

        if terminated or truncated:
            obs, info = env.reset()

    env.close()

if __name__ == "__main__":
    trainNeew = input("New training session? [y,n]:")=='y'
    print(trainNeew)

    if(not trainNeew):
        model = load_model("ppo_dynamic_target_cartpole")
        run_model(model,1000)
    else:
        model = training_session(timesteps=40000)
        run_model(model,1000)